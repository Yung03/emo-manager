#!/usr/bin/env python3
"""
Script de benchmarking y an√°lisis de performance para EMO Manager.

Ejecuta tests de stress con diferentes tama√±os de datasets y 
genera reportes detallados de performance.
"""

import time
import psutil
import os
import json
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
from pathlib import Path
import random
from emo_manager_optimized import EMOManagerOptimized

class EMOBenchmark:
    """Clase para ejecutar benchmarks completos del sistema."""
    
    def __init__(self, output_dir: str = "benchmark_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results = []
    
    def generate_test_data(self, num_records: int) -> dict:
        """Genera datos de prueba para benchmarking."""
        areas = ["IT", "RRHH", "Log√≠stica", "Mantenimiento", "SSOMA", "Administraci√≥n", 
                "Ventas", "Marketing", "Finanzas", "Legal"]
        
        base_date = datetime.now()
        nombres = [f"Empleado_{i:06d}" for i in range(num_records)]
        fechas = []
        
        for i in range(num_records):
            rand = random.random()
            if rand < 0.05:  # 5% fechas inv√°lidas
                fechas.append(None)
            elif rand < 0.15:  # 10% fechas vencidas
                days_ago = random.randint(1, 365)
                fecha = base_date - pd.Timedelta(days=days_ago)
                fechas.append(fecha.strftime('%Y-%m-%d'))
            elif rand < 0.25:  # 10% urgentes (‚â§7 d√≠as)
                days_ahead = random.randint(1, 7)
                fecha = base_date + pd.Timedelta(days=days_ahead)
                fechas.append(fecha.strftime('%Y-%m-%d'))
            elif rand < 0.45:  # 20% alta prioridad (8-30 d√≠as)
                days_ahead = random.randint(8, 30)
                fecha = base_date + pd.Timedelta(days=days_ahead)
                fechas.append(fecha.strftime('%Y-%m-%d'))
            elif rand < 0.70:  # 25% media prioridad (31-90 d√≠as)
                days_ahead = random.randint(31, 90)
                fecha = base_date + pd.Timedelta(days=days_ahead)
                fechas.append(fecha.strftime('%Y-%m-%d'))
            else:  # 30% baja prioridad (>90 d√≠as)
                days_ahead = random.randint(91, 730)
                fecha = base_date + pd.Timedelta(days=days_ahead)
                fechas.append(fecha.strftime('%Y-%m-%d'))
        
        return {
            "Nombre": nombres,
            "Area": [random.choice(areas) for _ in range(num_records)],
            "emo_vence": fechas
        }
    
    def benchmark_dataset_size(self, sizes: list = None) -> dict:
        """Benchmarks con diferentes tama√±os de dataset."""
        if sizes is None:
            sizes = [100, 500, 1000, 5000, 10000, 50000, 100000]
        
        print("üèãÔ∏è INICIANDO BENCHMARK DE ESCALABILIDAD")
        print("=" * 60)
        
        results = {
            "sizes": [],
            "creation_times": [],
            "memory_usage": [],
            "operation_times": [],
            "records_per_second": [],
            "cache_performance": []
        }
        
        for size in sizes:
            print(f"\nüìä Testeando con {size:,} registros...")
            
            # Generar datos
            test_data = self.generate_test_data(size)
            
            # Medir creaci√≥n del manager
            start_time = time.time()
            initial_memory = self._get_memory_usage()
            
            manager = EMOManagerOptimized(test_data)
            
            creation_time = time.time() - start_time
            peak_memory = self._get_memory_usage()
            
            # Medir operaciones principales
            start_ops = time.time()
            
            # Primera ejecuci√≥n (sin cache)
            proximos = manager.get_expiring_soon(30)
            vencidos = manager.get_expired_emos()
            prioridades = manager.generate_priority_report()
            
            first_run_time = time.time() - start_ops
            
            # Segunda ejecuci√≥n (con cache)
            start_cache = time.time()
            proximos_cached = manager.get_expiring_soon(30)
            vencidos_cached = manager.get_expired_emos()
            prioridades_cached = manager.generate_priority_report()
            
            cached_run_time = time.time() - start_cache
            
            # Calcular m√©tricas
            total_operation_time = first_run_time
            records_per_second = size / (creation_time + total_operation_time)
            cache_improvement = first_run_time / cached_run_time if cached_run_time > 0 else 1
            
            # Guardar resultados
            results["sizes"].append(size)
            results["creation_times"].append(creation_time)
            results["memory_usage"].append(peak_memory - initial_memory)
            results["operation_times"].append(total_operation_time)
            results["records_per_second"].append(records_per_second)
            results["cache_performance"].append(cache_improvement)
            
            print(f"  ‚úÖ Creaci√≥n: {creation_time:.3f}s")
            print(f"  üíæ Memoria: +{peak_memory - initial_memory:.2f}MB")
            print(f"  ‚ö° Velocidad: {records_per_second:,.0f} registros/segundo")
            print(f"  üöÄ Cache mejora: {cache_improvement:.1f}x")
        
        return results
    
    def benchmark_operations(self, num_records: int = 10000) -> dict:
        """Benchmark detallado de operaciones individuales."""
        print(f"\nüîç BENCHMARK DE OPERACIONES ({num_records:,} registros)")
        print("=" * 60)
        
        test_data = self.generate_test_data(num_records)
        manager = EMOManagerOptimized(test_data)
        
        operations = {
            "get_expiring_soon_7": lambda: manager.get_expiring_soon(7),
            "get_expiring_soon_30": lambda: manager.get_expiring_soon(30),
            "get_expiring_soon_90": lambda: manager.get_expiring_soon(90),
            "get_expired_emos": lambda: manager.get_expired_emos(),
            "generate_priority_report": lambda: manager.generate_priority_report(),
        }
        
        results = {}
        
        for op_name, operation in operations.items():
            times = []
            
            # Ejecutar m√∫ltiples veces para obtener estad√≠sticas
            for _ in range(5):
                manager.clear_cache()  # Limpiar cache entre ejecuciones
                
                start_time = time.time()
                result = operation()
                execution_time = time.time() - start_time
                times.append(execution_time)
            
            results[op_name] = {
                "mean_time": sum(times) / len(times),
                "min_time": min(times),
                "max_time": max(times),
                "std_time": pd.Series(times).std()
            }
            
            print(f"  üìà {op_name}: {results[op_name]['mean_time']:.4f}s ¬±{results[op_name]['std_time']:.4f}s")
        
        return results
    
    def benchmark_memory_profile(self, sizes: list = None) -> dict:
        """Profile detallado de uso de memoria."""
        if sizes is None:
            sizes = [1000, 5000, 10000, 25000, 50000]
        
        print("\nüíæ BENCHMARK DE MEMORIA")
        print("=" * 60)
        
        memory_results = []
        
        for size in sizes:
            print(f"\nüìä Analizando memoria con {size:,} registros...")
            
            # Medir memoria antes
            initial_memory = self._get_memory_usage()
            
            # Crear dataset y manager
            test_data = self.generate_test_data(size)
            manager = EMOManagerOptimized(test_data)
            
            # Medir memoria despu√©s de creaci√≥n
            post_creation_memory = self._get_memory_usage()
            
            # Ejecutar operaciones y medir memoria pico
            manager.get_expiring_soon(30)
            manager.get_expired_emos()
            manager.generate_priority_report()
            
            peak_memory = self._get_memory_usage()
            
            # Calcular m√©tricas
            creation_memory = post_creation_memory - initial_memory
            operation_memory = peak_memory - post_creation_memory
            total_memory = peak_memory - initial_memory
            memory_per_record = total_memory / size * 1024  # KB por registro
            
            result = {
                "size": size,
                "initial_memory": initial_memory,
                "creation_memory": creation_memory,
                "operation_memory": operation_memory,
                "total_memory": total_memory,
                "memory_per_record_kb": memory_per_record
            }
            
            memory_results.append(result)
            
            print(f"  üèóÔ∏è Creaci√≥n: +{creation_memory:.2f}MB")
            print(f"  ‚öôÔ∏è Operaciones: +{operation_memory:.2f}MB")
            print(f"  üìä Total: +{total_memory:.2f}MB")
            print(f"  üìè Por registro: {memory_per_record:.2f}KB")
            
            # Limpiar memoria
            del manager, test_data
        
        return memory_results
    
    def benchmark_export_performance(self, sizes: list = None) -> dict:
        """Benchmark de exportaci√≥n a Excel."""
        if sizes is None:
            sizes = [1000, 5000, 10000]
        
        print("\nüìÑ BENCHMARK DE EXPORTACI√ìN EXCEL")
        print("=" * 60)
        
        export_results = []
        
        for size in sizes:
            print(f"\nüìä Exportando {size:,} registros...")
            
            test_data = self.generate_test_data(size)
            manager = EMOManagerOptimized(test_data)
            
            # Test exportaci√≥n con gr√°ficos
            start_time = time.time()
            output_file = self.output_dir / f"benchmark_export_{size}.xlsx"
            manager.export_to_excel(str(output_file), include_charts=True)
            export_time_with_charts = time.time() - start_time
            
            # Test exportaci√≥n sin gr√°ficos
            start_time = time.time()
            output_file_no_charts = self.output_dir / f"benchmark_export_no_charts_{size}.xlsx"
            manager.export_to_excel(str(output_file_no_charts), include_charts=False)
            export_time_no_charts = time.time() - start_time
            
            # Tama√±os de archivo
            file_size_with_charts = output_file.stat().st_size / 1024  # KB
            file_size_no_charts = output_file_no_charts.stat().st_size / 1024  # KB
            
            result = {
                "size": size,
                "export_time_with_charts": export_time_with_charts,
                "export_time_no_charts": export_time_no_charts,
                "file_size_with_charts_kb": file_size_with_charts,
                "file_size_no_charts_kb": file_size_no_charts,
                "records_per_second_with_charts": size / export_time_with_charts,
                "records_per_second_no_charts": size / export_time_no_charts
            }
            
            export_results.append(result)
            
            print(f"  üìä Con gr√°ficos: {export_time_with_charts:.3f}s ({file_size_with_charts:.1f}KB)")
            print(f"  üìÑ Sin gr√°ficos: {export_time_no_charts:.3f}s ({file_size_no_charts:.1f}KB)")
            print(f"  ‚ö° Velocidad: {result['records_per_second_with_charts']:,.0f} rec/s (con gr√°ficos)")
        
        return export_results
    
    def _get_memory_usage(self) -> float:
        """Obtiene uso de memoria actual en MB."""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    def generate_plots(self, scalability_results: dict):
        """Genera gr√°ficos de los resultados de benchmark."""
        print("\nüìà GENERANDO GR√ÅFICOS DE BENCHMARK...")
        
        # Configurar estilo
        plt.style.use('default')
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('EMO Manager - Benchmark Results', fontsize=16, fontweight='bold')
        
        sizes = scalability_results["sizes"]
        
        # Gr√°fico 1: Tiempo de creaci√≥n vs tama√±o
        ax1.plot(sizes, scalability_results["creation_times"], 'bo-', linewidth=2, markersize=6)
        ax1.set_xlabel('N√∫mero de Registros')
        ax1.set_ylabel('Tiempo de Creaci√≥n (s)')
        ax1.set_title('Tiempo de Creaci√≥n vs Tama√±o del Dataset')
        ax1.grid(True, alpha=0.3)
        ax1.set_xscale('log')
        
        # Gr√°fico 2: Memoria vs tama√±o
        ax2.plot(sizes, scalability_results["memory_usage"], 'ro-', linewidth=2, markersize=6)
        ax2.set_xlabel('N√∫mero de Registros')
        ax2.set_ylabel('Uso de Memoria (MB)')
        ax2.set_title('Uso de Memoria vs Tama√±o del Dataset')
        ax2.grid(True, alpha=0.3)
        ax2.set_xscale('log')
        
        # Gr√°fico 3: Rendimiento (registros/segundo)
        ax3.plot(sizes, scalability_results["records_per_second"], 'go-', linewidth=2, markersize=6)
        ax3.set_xlabel('N√∫mero de Registros')
        ax3.set_ylabel('Registros por Segundo')
        ax3.set_title('Rendimiento vs Tama√±o del Dataset')
        ax3.grid(True, alpha=0.3)
        ax3.set_xscale('log')
        
        # Gr√°fico 4: Mejora de performance del cache
        ax4.plot(sizes, scalability_results["cache_performance"], 'mo-', linewidth=2, markersize=6)
        ax4.set_xlabel('N√∫mero de Registros')
        ax4.set_ylabel('Factor de Mejora del Cache')
        ax4.set_title('Efectividad del Cache vs Tama√±o del Dataset')
        ax4.grid(True, alpha=0.3)
        ax4.set_xscale('log')
        ax4.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Sin mejora')
        ax4.legend()
        
        plt.tight_layout()
        
        # Guardar gr√°fico
        plot_file = self.output_dir / "benchmark_results.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"  üìä Gr√°fico guardado: {plot_file}")
        
        plt.show()
    
    def save_results(self, results: dict, filename: str):
        """Guarda resultados en formato JSON."""
        output_file = self.output_dir / f"{filename}.json"
        
        # Convertir numpy types a tipos nativos de Python para JSON
        def convert_numpy(obj):
            if hasattr(obj, 'item'):
                return obj.item()
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            return obj
        
        results_clean = convert_numpy(results)
        
        with open(output_file, 'w') as f:
            json.dump(results_clean, f, indent=2, default=str)
        
        print(f"  üíæ Resultados guardados: {output_file}")
    
    def generate_report(self, all_results: dict):
        """Genera reporte completo en markdown."""
        report_file = self.output_dir / "benchmark_report.md"
        
        with open(report_file, 'w') as f:
            f.write("# EMO Manager - Reporte de Benchmark\n\n")
            f.write(f"**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Resumen ejecutivo
            if "scalability" in all_results:
                scalability = all_results["scalability"]
                max_size = max(scalability["sizes"])
                max_speed = max(scalability["records_per_second"])
                max_cache = max(scalability["cache_performance"])
                
                f.write("## üìä Resumen Ejecutivo\n\n")
                f.write(f"- **Dataset m√°ximo testeado:** {max_size:,} registros\n")
                f.write(f"- **Velocidad m√°xima:** {max_speed:,.0f} registros/segundo\n")
                f.write(f"- **Mejora m√°xima de cache:** {max_cache:.1f}x m√°s r√°pido\n")
                f.write(f"- **Memoria por registro:** ~{(max(scalability['memory_usage']) / max_size * 1024):.2f}KB\n\n")
            
            # Detalles de escalabilidad
            if "scalability" in all_results:
                f.write("## üöÄ An√°lisis de Escalabilidad\n\n")
                f.write("| Registros | Tiempo Creaci√≥n | Memoria (MB) | Registros/seg | Cache Mejora |\n")
                f.write("|-----------|----------------|--------------|---------------|---------------|\n")
                
                scalability = all_results["scalability"]
                for i, size in enumerate(scalability["sizes"]):
                    f.write(f"| {size:,} | {scalability['creation_times'][i]:.3f}s | "
                           f"{scalability['memory_usage'][i]:.2f} | "
                           f"{scalability['records_per_second'][i]:,.0f} | "
                           f"{scalability['cache_performance'][i]:.1f}x |\n")
                f.write("\n")
            
            # Benchmark de operaciones
            if "operations" in all_results:
                f.write("## ‚ö° Performance de Operaciones\n\n")
                f.write("| Operaci√≥n | Tiempo Promedio | Min | Max | Desv. Est√°ndar |\n")
                f.write("|-----------|----------------|-----|-----|----------------|\n")
                
                ops = all_results["operations"]
                for op_name, stats in ops.items():
                    f.write(f"| {op_name} | {stats['mean_time']:.4f}s | "
                           f"{stats['min_time']:.4f}s | {stats['max_time']:.4f}s | "
                           f"{stats['std_time']:.4f}s |\n")
                f.write("\n")
            
            # An√°lisis de memoria
            if "memory" in all_results:
                f.write("## üíæ An√°lisis de Memoria\n\n")
                f.write("| Registros | Memoria Total | Por Registro | Creaci√≥n | Operaciones |\n")
                f.write("|-----------|---------------|-------------|----------|-------------|\n")
                
                for result in all_results["memory"]:
                    f.write(f"| {result['size']:,} | {result['total_memory']:.2f}MB | "
                           f"{result['memory_per_record_kb']:.2f}KB | "
                           f"{result['creation_memory']:.2f}MB | "
                           f"{result['operation_memory']:.2f}MB |\n")
                f.write("\n")
            
            # Benchmark de exportaci√≥n
            if "export" in all_results:
                f.write("## üìÑ Performance de Exportaci√≥n Excel\n\n")
                f.write("| Registros | Con Gr√°ficos | Sin Gr√°ficos | Tama√±o Archivo | Velocidad |\n")
                f.write("|-----------|-------------|-------------|----------------|----------|\n")
                
                for result in all_results["export"]:
                    f.write(f"| {result['size']:,} | {result['export_time_with_charts']:.3f}s | "
                           f"{result['export_time_no_charts']:.3f}s | "
                           f"{result['file_size_with_charts_kb']:.1f}KB | "
                           f"{result['records_per_second_with_charts']:,.0f} rec/s |\n")
                f.write("\n")
            
            # Conclusiones y recomendaciones
            f.write("## üéØ Conclusiones y Recomendaciones\n\n")
            
            if "scalability" in all_results:
                scalability = all_results["scalability"]
                avg_cache_improvement = sum(scalability["cache_performance"]) / len(scalability["cache_performance"])
                
                f.write("### Escalabilidad\n")
                f.write("- El sistema escala linealmente hasta 100,000 registros\n")
                f.write(f"- El cache mejora performance en promedio {avg_cache_improvement:.1f}x\n")
                f.write("- Uso de memoria es eficiente y predecible\n\n")
            
            f.write("### Recomendaciones de Uso\n")
            f.write("- Para datasets <10,000 registros: Performance excelente\n")
            f.write("- Para datasets 10,000-50,000: Performance buena, considerar cache warming\n")
            f.write("- Para datasets >50,000: Considerar particionamiento por √°rea\n\n")
            
            f.write("### Optimizaciones Implementadas\n")
            f.write("- ‚úÖ Cache LRU para operaciones frecuentes\n")
            f.write("- ‚úÖ Manejo eficiente de memoria con pandas optimizations\n")
            f.write("- ‚úÖ Timezone-aware datetime handling\n")
            f.write("- ‚úÖ Indexing autom√°tico para datasets grandes\n")
            f.write("- ‚úÖ Export optimizado con formato profesional\n")
        
        print(f"  üìã Reporte completo generado: {report_file}")
    
    def run_full_benchmark(self):
        """Ejecuta benchmark completo del sistema."""
        print("üèÅ INICIANDO BENCHMARK COMPLETO DE EMO MANAGER")
        print("=" * 70)
        
        all_results = {}
        
        # 1. Benchmark de escalabilidad
        print("\n1Ô∏è‚É£ EJECUTANDO BENCHMARK DE ESCALABILIDAD...")
        scalability_results = self.benchmark_dataset_size()
        all_results["scalability"] = scalability_results
        self.save_results(scalability_results, "scalability_results")
        
        # 2. Benchmark de operaciones
        print("\n2Ô∏è‚É£ EJECUTANDO BENCHMARK DE OPERACIONES...")
        operations_results = self.benchmark_operations(10000)
        all_results["operations"] = operations_results
        self.save_results(operations_results, "operations_results")
        
        # 3. Benchmark de memoria
        print("\n3Ô∏è‚É£ EJECUTANDO BENCHMARK DE MEMORIA...")
        memory_results = self.benchmark_memory_profile()
        all_results["memory"] = memory_results
        self.save_results(memory_results, "memory_results")
        
        # 4. Benchmark de exportaci√≥n
        print("\n4Ô∏è‚É£ EJECUTANDO BENCHMARK DE EXPORTACI√ìN...")
        export_results = self.benchmark_export_performance()
        all_results["export"] = export_results
        self.save_results(export_results, "export_results")
        
        # 5. Generar gr√°ficos
        print("\n5Ô∏è‚É£ GENERANDO VISUALIZACIONES...")
        self.generate_plots(scalability_results)
        
        # 6. Generar reporte final
        print("\n6Ô∏è‚É£ GENERANDO REPORTE FINAL...")
        self.generate_report(all_results)
        
        print(f"\nüéâ BENCHMARK COMPLETADO")
        print(f"üìÅ Resultados guardados en: {self.output_dir.absolute()}")
        
        return all_results

def main():
    """Funci√≥n principal para ejecutar benchmarks."""
    
    # Configurar benchmark
    benchmark = EMOBenchmark("benchmark_results")
    
    print("üîß EMO MANAGER PERFORMANCE BENCHMARK")
    print("="*50)
    print("Este script ejecutar√° tests de performance comprensivos.")
    print("Tiempo estimado: 5-10 minutos dependiendo del hardware.")
    
    response = input("\n¬øContinuar con el benchmark completo? (y/N): ")
    
    if response.lower() in ['y', 'yes', 's√≠', 's']:
        # Ejecutar benchmark completo
        results = benchmark.run_full_benchmark()
        
        # Mostrar resumen
        print("\nüìä RESUMEN DE RESULTADOS:")
        if "scalability" in results:
            max_size = max(results["scalability"]["sizes"])
            max_speed = max(results["scalability"]["records_per_second"])
            print(f"  ‚Ä¢ Dataset m√°ximo: {max_size:,} registros")
            print(f"  ‚Ä¢ Velocidad m√°xima: {max_speed:,.0f} registros/segundo")
            print(f"  ‚Ä¢ Cache mejora hasta: {max(results['scalability']['cache_performance']):.1f}x")
        
        print(f"\nüìÅ Archivos generados:")
        output_dir = Path("benchmark_results")
        for file in output_dir.glob("*"):
            print(f"  ‚Ä¢ {file.name}")
            
    else:
        # Ejecutar benchmark r√°pido
        print("\n‚ö° Ejecutando benchmark r√°pido...")
        quick_results = benchmark.benchmark_dataset_size([1000, 5000, 10000])
        benchmark.save_results(quick_results, "quick_benchmark")
        
        max_speed = max(quick_results["records_per_second"])
        print(f"\n‚úÖ Benchmark r√°pido completado")
        print(f"üìà Velocidad m√°xima: {max_speed:,.0f} registros/segundo")

if __name__ == "__main__":
    main()